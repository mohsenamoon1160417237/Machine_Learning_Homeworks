{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a211b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip3 install xgboost\n",
    "# !pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fdc5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b7c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "#Q1\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # For multi-class classification\n",
    "    'num_class': 10,                # 10 digits (0-9)\n",
    "    'eval_metric': 'merror',       # Multiclass error rate\n",
    "    'eta': 0.1,                     # Learning rate\n",
    "    'max_depth': 6,                # Maximum depth of trees\n",
    "    'subsample': 0.8,              # Subsample ratio of training instances\n",
    "    'colsample_bytree': 0.8,       # Subsample ratio of features\n",
    "    'seed': 42                      # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 5  # Adjust as needed\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    return X - mean\n",
    "\n",
    "def covariance_matrix(X):\n",
    "    n_samples = X.shape[0]\n",
    "    return (1 / (n_samples - 1)) * np.dot(X.T, X)\n",
    "\n",
    "def eigen_decomposition(covariance):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def select_principal_components(eigenvalues, eigenvectors, n_components):\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    return eigenvectors[:, :n_components]\n",
    "\n",
    "def transform_data(X, principal_components):\n",
    "    return np.dot(X, principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b383b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original data: (70000, 784)\n",
      "Shape of transformed data: (56000, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Load and preprocess data (same as before)\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_centered = center_data(X_train)\n",
    "X_test_centered = center_data(X_test)\n",
    "\n",
    "n_components = 50  # Number of principal components to keep\n",
    "\n",
    "covariance = covariance_matrix(X_train_centered)\n",
    "eigenvalues, eigenvectors = eigen_decomposition(covariance)\n",
    "principal_components = select_principal_components(eigenvalues, eigenvectors, n_components)\n",
    "# principal_components = PCA(n_components=n_components)\n",
    "X_train_pca = transform_data(X_train_centered, principal_components)\n",
    "X_test_pca = transform_data(X_test_centered, principal_components)\n",
    "# X_train_pca = principal_components.fit_transform(X_train_centered)\n",
    "# X_test_pca = principal_components.fit_transform(X_test_centered)\n",
    "\n",
    "\n",
    "print(\"Shape of original data:\", X.shape)\n",
    "print(\"Shape of transformed data:\", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ba246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (PCA with 50 components): 0.8785\n"
     ]
    }
   ],
   "source": [
    "# Convert to DMatrix\n",
    "#Q2 - a\n",
    "dtrain_pca = xgb.DMatrix(data=X_train_pca, label=y_train)\n",
    "dtest_pca = xgb.DMatrix(data=X_test_pca, label=y_test)\n",
    "\n",
    "# XGBoost parameters (adjust as needed)\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 10,\n",
    "    'eval_metric': 'merror',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train XGBoost model\n",
    "num_rounds = 5\n",
    "model_pca = xgb.train(params, dtrain_pca, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_pca = model_pca.predict(dtest_pca)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"Test Accuracy (PCA with 50 components): {accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c9864fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Class-Conditional PCA with 50 components per class): 0.9627\n"
     ]
    }
   ],
   "source": [
    "#Q2 - b\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# --- MNIST Data Loading and Preprocessing ---\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Class-Conditional PCA ---\n",
    "n_components = 50\n",
    "class_data = defaultdict(list)\n",
    "\n",
    "for i, label in enumerate(y_train):\n",
    "#     print(i, label)\n",
    "#     break\n",
    "    class_data[label].append(X_train.iloc[i])\n",
    "\n",
    "principal_components_per_class = {}\n",
    "for digit, data in class_data.items():\n",
    "    data_array = np.array(data)\n",
    "    data_centered = center_data(data_array)\n",
    "    covariance = covariance_matrix(data_centered)\n",
    "    eigenvalues, eigenvectors = eigen_decomposition(covariance)\n",
    "    principal_components_per_class[digit] = select_principal_components(eigenvalues, eigenvectors, n_components)\n",
    "\n",
    "# Transform data\n",
    "X_train_transformed = np.zeros((len(X_train), n_components))\n",
    "for i, label in enumerate(y_train):\n",
    "#     df_i = pd.DataFrame(X_train.iloc[i], columns=X_train.columns)\n",
    "    X_train_transformed[i] = transform_data(X_train.iloc[i].values.reshape(1,-1), principal_components_per_class[label]).flatten()\n",
    "\n",
    "X_test_transformed = np.zeros((len(X_test), n_components))\n",
    "for i, label in enumerate(y_test):\n",
    "#     df_i = pd.DataFrame(X_test.iloc[i], columns=X_train.columns)\n",
    "    X_test_transformed[i] = transform_data(X_test.iloc[i].values.reshape(1,-1), principal_components_per_class[label]).flatten()\n",
    "\n",
    "# --- XGBoost Training and Evaluation ---\n",
    "dtrain_pca = xgb.DMatrix(data=X_train_transformed, label=y_train)\n",
    "dtest_pca = xgb.DMatrix(data=X_test_transformed, label=y_test)\n",
    "\n",
    "params = {  #Keep the same XGBoost parameters as before\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 10,\n",
    "    'eval_metric': 'merror',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "num_rounds = 5\n",
    "model_pca = xgb.train(params, dtrain_pca, num_rounds)\n",
    "y_pred_pca = model_pca.predict(dtest_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"Test Accuracy (Class-Conditional PCA with 50 components per class): {accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a1c08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3572\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "x_train = x_train.reshape(-1,32* 32*3)\n",
    "x_test = x_test.reshape(-1,32* 32 *3)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 10,\n",
    "    'eval_metric': 'merror',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test, label=y_test)\n",
    "\n",
    "num_round = 10  # Adjust as needed\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=num_round)\n",
    "\n",
    "predictions = model.predict(dtest)\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80fee5b8-7b1f-461d-8a28-376e2c8f6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.67160\n",
      "[1]\teval-merror:0.64560\n",
      "[2]\teval-merror:0.63230\n",
      "[3]\teval-merror:0.62450\n",
      "[4]\teval-merror:0.61790\n",
      "[5]\teval-merror:0.61190\n",
      "[6]\teval-merror:0.61150\n",
      "[7]\teval-merror:0.60520\n",
      "[8]\teval-merror:0.60360\n",
      "[9]\teval-merror:0.60050\n",
      "[10]\teval-merror:0.59870\n",
      "[11]\teval-merror:0.59470\n",
      "[12]\teval-merror:0.59270\n",
      "[13]\teval-merror:0.59130\n",
      "[14]\teval-merror:0.59020\n",
      "[15]\teval-merror:0.58710\n",
      "[16]\teval-merror:0.58610\n",
      "[17]\teval-merror:0.58500\n",
      "[18]\teval-merror:0.58270\n",
      "[19]\teval-merror:0.58130\n",
      "[20]\teval-merror:0.58060\n",
      "[21]\teval-merror:0.57670\n",
      "[22]\teval-merror:0.57510\n",
      "[23]\teval-merror:0.57440\n",
      "[24]\teval-merror:0.57240\n",
      "[25]\teval-merror:0.57270\n",
      "[26]\teval-merror:0.57060\n",
      "[27]\teval-merror:0.56880\n",
      "[28]\teval-merror:0.56920\n",
      "[29]\teval-merror:0.56770\n",
      "[30]\teval-merror:0.56650\n",
      "[31]\teval-merror:0.56470\n",
      "[32]\teval-merror:0.56290\n",
      "[33]\teval-merror:0.56320\n",
      "[34]\teval-merror:0.56320\n",
      "[35]\teval-merror:0.56190\n",
      "[36]\teval-merror:0.56150\n",
      "[37]\teval-merror:0.56110\n",
      "[38]\teval-merror:0.56080\n",
      "[39]\teval-merror:0.55890\n",
      "[40]\teval-merror:0.55810\n",
      "[41]\teval-merror:0.55880\n",
      "[42]\teval-merror:0.55870\n",
      "[43]\teval-merror:0.55710\n",
      "[44]\teval-merror:0.55610\n",
      "[45]\teval-merror:0.55500\n",
      "[46]\teval-merror:0.55470\n",
      "[47]\teval-merror:0.55560\n",
      "[48]\teval-merror:0.55410\n",
      "[49]\teval-merror:0.55280\n",
      "[50]\teval-merror:0.55280\n",
      "[51]\teval-merror:0.55160\n",
      "[52]\teval-merror:0.55210\n",
      "[53]\teval-merror:0.55110\n",
      "[54]\teval-merror:0.54900\n",
      "[55]\teval-merror:0.54980\n",
      "[56]\teval-merror:0.54950\n",
      "[57]\teval-merror:0.54980\n",
      "[58]\teval-merror:0.54840\n",
      "[59]\teval-merror:0.54830\n",
      "[60]\teval-merror:0.54680\n",
      "[61]\teval-merror:0.54600\n",
      "[62]\teval-merror:0.54620\n",
      "[63]\teval-merror:0.54450\n",
      "[64]\teval-merror:0.54480\n",
      "[65]\teval-merror:0.54240\n",
      "[66]\teval-merror:0.54100\n",
      "[67]\teval-merror:0.54070\n",
      "[68]\teval-merror:0.53970\n",
      "[69]\teval-merror:0.53940\n",
      "[70]\teval-merror:0.53850\n",
      "[71]\teval-merror:0.53700\n",
      "[72]\teval-merror:0.53590\n",
      "[73]\teval-merror:0.53500\n",
      "[74]\teval-merror:0.53490\n",
      "[75]\teval-merror:0.53590\n",
      "[76]\teval-merror:0.53500\n",
      "[77]\teval-merror:0.53490\n",
      "[78]\teval-merror:0.53550\n",
      "[79]\teval-merror:0.53450\n",
      "[80]\teval-merror:0.53430\n",
      "[81]\teval-merror:0.53320\n",
      "[82]\teval-merror:0.53280\n",
      "[83]\teval-merror:0.53180\n",
      "[84]\teval-merror:0.53060\n",
      "[85]\teval-merror:0.53040\n",
      "[86]\teval-merror:0.52940\n",
      "[87]\teval-merror:0.52850\n",
      "[88]\teval-merror:0.52910\n",
      "[89]\teval-merror:0.52930\n",
      "[90]\teval-merror:0.53000\n",
      "[91]\teval-merror:0.52900\n",
      "[92]\teval-merror:0.52860\n",
      "[93]\teval-merror:0.52950\n",
      "[94]\teval-merror:0.52860\n",
      "[95]\teval-merror:0.52800\n",
      "[96]\teval-merror:0.52740\n",
      "[97]\teval-merror:0.52670\n",
      "[98]\teval-merror:0.52650\n",
      "[99]\teval-merror:0.52710\n",
      "Test Accuracy (PCA with 50 components): 0.4715\n"
     ]
    }
   ],
   "source": [
    "#Q4 - a\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "# Reshape to (samples, features)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Apply PCA\n",
    "n_components = 50\n",
    "covariance = covariance_matrix(x_train)\n",
    "eigenvalues, eigenvectors = eigen_decomposition(covariance)\n",
    "principal_components = select_principal_components(eigenvalues, eigenvectors, n_components)\n",
    "x_train_pca = transform_data(x_train, principal_components)\n",
    "x_test_pca = transform_data(x_test, principal_components)\n",
    "\n",
    "# Split data into training and validation sets (optional, for better evaluation)\n",
    "x_train_pca, x_val_pca, y_train, y_val = train_test_split(x_train_pca, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 10,\n",
    "    'eval_metric': 'merror',  # Use multiclass classification error\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,  # Experiment with different depths\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Create DMatrices\n",
    "dtrain = xgb.DMatrix(data=x_train_pca, label=y_train)\n",
    "dval = xgb.DMatrix(data=x_val_pca, label=y_val)  # Validation set\n",
    "dtest = xgb.DMatrix(data=x_test_pca, label=y_test)\n",
    "\n",
    "# Train XGBoost model with early stopping\n",
    "num_round = 100  # Experiment with the number of rounds\n",
    "evallist = [(dval, 'eval')]  # Use validation set for early stopping\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=num_round, evals=evallist, early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy (PCA with {n_components} components): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "441fde58-3996-4c37-bcc7-6860a2e2601c",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[17:42:10] /Users/runner/work/xgboost/xgboost/src/data/data.cc:508: Check failed: this->labels.Size() % this->num_row_ == 0 (10000 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000162638428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016279635c xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 2532\n  [bt] (2) 3   libxgboost.dylib                    0x00000001627957ec xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 464\n  [bt] (3) 4   libxgboost.dylib                    0x000000016264fa60 XGDMatrixSetInfoFromInterface + 228\n  [bt] (4) 5   libffi.dylib                        0x00000001b9c00050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001b9c089e4 ffi_call_int + 948\n  [bt] (6) 7   _ctypes.cpython-39-darwin.so        0x00000001051ce944 _ctypes_callproc + 1404\n  [bt] (7) 8   _ctypes.cpython-39-darwin.so        0x00000001051c7c10 PyCFuncPtr_call + 1168\n  [bt] (8) 9   Python                              0x0000000102a90560 _PyObject_MakeTpCall + 360\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 49\u001b[0m\n\u001b[1;32m     37\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Create DMatrices\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m dval \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(data\u001b[38;5;241m=\u001b[39mx_val_pca, label\u001b[38;5;241m=\u001b[39my_val)  \u001b[38;5;66;03m# Validation set\u001b[39;00m\n\u001b[1;32m     51\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(data\u001b[38;5;241m=\u001b[39mx_test_pca, label\u001b[38;5;241m=\u001b[39my_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:890\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m feature_names\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:954\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:1092\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m-> 1092\u001b[0m \u001b[43mdispatch_meta_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/data.py:1338\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[0;32m-> 1338\u001b[0m     \u001b[43m_meta_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_arrow(data):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/data.py:1279\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked array is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1278\u001b[0m interface_str \u001b[38;5;241m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1279\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixSetInfoFromInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [17:42:10] /Users/runner/work/xgboost/xgboost/src/data/data.cc:508: Check failed: this->labels.Size() % this->num_row_ == 0 (10000 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000162638428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016279635c xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 2532\n  [bt] (2) 3   libxgboost.dylib                    0x00000001627957ec xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 464\n  [bt] (3) 4   libxgboost.dylib                    0x000000016264fa60 XGDMatrixSetInfoFromInterface + 228\n  [bt] (4) 5   libffi.dylib                        0x00000001b9c00050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001b9c089e4 ffi_call_int + 948\n  [bt] (6) 7   _ctypes.cpython-39-darwin.so        0x00000001051ce944 _ctypes_callproc + 1404\n  [bt] (7) 8   _ctypes.cpython-39-darwin.so        0x00000001051c7c10 PyCFuncPtr_call + 1168\n  [bt] (8) 9   Python                              0x0000000102a90560 _PyObject_MakeTpCall + 360\n\n"
     ]
    }
   ],
   "source": [
    "#Q4 - b\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "# Reshape to (samples, features)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Separate data by class\n",
    "n_classes = 10\n",
    "x_train_by_class = [x_train[y_train == i] for i in range(n_classes)]\n",
    "\n",
    "# Apply PCA per class\n",
    "n_components = 50\n",
    "pca_models = []\n",
    "x_train_pca = np.zeros((len(x_train), n_components))\n",
    "for i in range(n_classes):\n",
    "    covariance = covariance_matrix(x_train_by_class[i])\n",
    "    eigenvalues, eigenvectors = eigen_decomposition(covariance)\n",
    "    pca_models.append(select_principal_components(eigenvalues, eigenvectors, n_components))\n",
    "    # pca_models.append(transform_data(x_train_by_class[i], principal_components))\n",
    "    x_train_pca[y_train == i, :] = transform_data(x_train[y_train == i], pca_models[i])\n",
    "    # x_train_pca[y_train == i, :] = pca_models[i].transform(x_train[y_train == i])\n",
    "\n",
    "# Apply PCA to the test data using the fitted PCA models\n",
    "x_test_pca = np.zeros((len(x_test), n_components))\n",
    "for i in range(n_classes):\n",
    "    x_test_pca[y_test == i, :] = transform_data(x_test[y_test == i], pca_models[i])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train_pca, x_val_pca, y_train_split, y_val = train_test_split(x_train_pca, y_train, test_size=0.2, random_state=42)\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 10,\n",
    "    'eval_metric': 'merror',  # Use multiclass classification error\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,  # Experiment with different depths\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Create DMatrices\n",
    "dtrain = xgb.DMatrix(data=x_train_pca, label=y_train)\n",
    "dval = xgb.DMatrix(data=x_val_pca, label=y_val)  # Validation set\n",
    "dtest = xgb.DMatrix(data=x_test_pca, label=y_test)\n",
    "\n",
    "# Train XGBoost model with early stopping\n",
    "num_round = 100  # Experiment with the number of rounds\n",
    "evallist = [(dval, 'eval')]  # Use validation set for early stopping\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=num_round, evals=evallist, early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy (PCA with {n_components} components): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73643c7-9862-49b5-a296-9d4aaee8436e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
